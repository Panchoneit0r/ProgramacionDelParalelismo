
# CUDA Execution Model
## INTRODUCING THE CUDA EXECUTION MODEL
La arquitectura de la GPU se basa en una matriz escalable de multiprocesadores de transmisión (SM). Cada SM está diseñado para admitir la ejecución simultánea de cientos de subprocesos. Cuando se inicia una cuadrícula del núcleo, los bloques de subprocesos de esa cuadrícula del núcleo se distribuyen entre los SM disponibles para su ejecución. Los subprocesos de un bloque de subprocesos se ejecutan simultáneamente solo en el SM asignado. La GPU emplea una arquitectura de subprocesos múltiples de instrucción única (SIMT), donde todos los subprocesos en una urdimbre ejecutan la misma instrucción al mismo tiempo.
La arquitectura Fermi es la primera arquitectura informática GPU completa que ofrece las funciones necesarias para aplicaciones exigentes de informática de alto rendimiento (HPC). Ha sido ampliamente adoptado para acelerar las cargas de trabajo de producción. Fermi cuenta con hasta 512 núcleos aceleradores llamados núcleos CUDA, organizados en 16 multiprocesadores de transmisión (SM) con 32 núcleos CUDA cada uno. También tiene seis interfaces de memoria DRAM GDDR5 de 384 bits que admiten hasta 6 GB de memoria integrada global. El motor GigaThread es un programador global que distribuye bloques de subprocesos a los programadores warp de SM.
La arquitectura Kepler es una arquitectura informática de alto rendimiento que ofrece características mejoradas para la informática híbrida. Incluye 15 multiprocesadores de transmisión (SM) y seis controladores de memoria de 64 bits. Tres innovaciones importantes en la arquitectura Kepler son SM mejorados, paralelismo dinámico e Hyper-Q.
SM mejorados: la arquitectura Kepler presenta SM mejorados que ofrecen un rendimiento mejorado en comparación con arquitecturas anteriores. Estos SM mejorados permiten una ejecución más eficiente de cargas de trabajo paralelas.
Paralelismo dinámico: las GPU Kepler introducen paralelismo dinámico, lo que permite a la GPU lanzar dinámicamente nuevas grillas y núcleos. Esta característica permite que la GPU inicie núcleos anidados sin la necesidad de comunicarse con la CPU, lo que facilita la creación y optimización de patrones de ejecución recursivos y dependientes de datos.
Hyper-Q: Hyper-Q es una característica que agrega más conexiones de hardware simultáneas entre la CPU y la GPU. Permite que los núcleos de la CPU ejecuten simultáneamente más tareas en la GPU, lo que aumenta la utilización de la GPU y reduce el tiempo de inactividad de la CPU. Las GPU Kepler proporcionan 32 colas de trabajo de hardware entre el host y la GPU, lo que permite una mayor concurrencia y maximiza el rendimiento general.

## Understanding the Nature of Warp Execution
En este capítulo, la atención se centra en la cuestión de la latencia de las instrucciones y las limitaciones de los recursos informáticos en el modelo de ejecución CUDA. Es importante que los programadores de CUDA C comprendan los recursos de hardware para mejorar el rendimiento del kernel. El capítulo explica el concepto de warps, que son la unidad básica de ejecución en un SM (Streaming Multiprocessor). También analiza la divergencia de deformación, que se produce cuando los subprocesos de una deformación ejecutan instrucciones diferentes, lo que provoca una degradación del rendimiento. El capítulo proporciona información sobre la ejecución de warp desde una perspectiva de hardware y enfatiza la necesidad de evitar diferentes rutas de ejecución dentro del mismo warp para un rendimiento óptimo. Además, introduce el concepto de warps activos y su clasificación como warps seleccionados, estancados o elegibles. El capítulo concluye discutiendo la importancia de exponer suficiente paralelismo para ocultar la latencia y lograr un mejor rendimiento. No abordó más en él pues en el resumen pasado ya se mencionaron estos temas de forma más extensa 

## Exposing Parallelism
El paralelismo dinámico permite la creación de nuevos trabajos directamente desde la GPU, lo que facilita la expresión de algoritmos paralelos recursivos o dependientes de datos de manera más natural. También menciona la importancia de considerar la estrategia de lanzamiento de la grilla secundaria, la sincronización padre-hijo y la profundidad de los niveles anidados al implementar un kernel anidado eficiente. Se destaca que la cantidad máxima de anidamientos del kernel puede ser limitada debido a la reserva de memoria adicional en cada nivel de anidamiento. Además, se enfatiza la importancia de la sincronización tanto para el rendimiento como para la corrección, y se sugiere que reducir la cantidad de sincronizaciones en bloque puede llevar a núcleos anidados más eficientes. En última instancia, se concluye que el paralelismo dinámico permite adaptarse a decisiones o cargas de trabajo basadas en datos al tomar decisiones de configuración de lanzamiento en tiempo de ejecución en el dispositivo.

## Avoid Branch Divergence
La divergencia de ramas ocurre cuando los subprocesos dentro de un warp toman diferentes rutas de código, lo que lleva a un rendimiento deficiente del kernel. Para evitar la divergencia de las ramas, se recomienda ajustar la granularidad de las ramas para que sea un múltiplo del tamaño de la deformación. Diferentes warps pueden ejecutar código diferente sin penalizar el rendimiento.
Una técnica para evitar la divergencia de ramas es utilizar una implementación iterativa por pares para la suma paralela. Esto implica dividir el vector de entrada en fragmentos más pequeños y hacer que un hilo calcule la suma parcial de cada fragmento. Los resultados parciales de cada fragmento se suman para obtener la suma final.
Otra técnica consiste en utilizar el enfoque de pares entrelazados, donde los elementos emparejados están separados por una zancada determinada. Este enfoque reduce la divergencia de la deformación al garantizar que todos los hilos dentro de una deformación tomen el mismo camino de control.
Es importante tener en cuenta que el compilador CUDA realiza optimizaciones para mantener la eficiencia de las sucursales por encima del 50 por ciento. Sin embargo, las rutas de código largas aún pueden provocar divergencias de deformación. Al comprender la naturaleza de la ejecución warp e implementar técnicas apropiadas, se puede minimizar la divergencia de ramas, lo que conduce a un mejor rendimiento del kernel.

## Unrolling Loops
El desenrollado de bucles es una técnica utilizada para mejorar el rendimiento de los bucles al reducir la sobrecarga de instrucciones y crear instrucciones más independientes. Es más eficaz para bucles de procesamiento de matrices secuenciales donde se conoce el número de iteraciones antes de la ejecución. Desarrollar bucles puede generar ganancias de rendimiento al reducir la cantidad de veces que se verifica la condición del bucle y permitir operaciones de memoria simultáneas. El desenrollado se puede realizar en diferentes niveles, como replicar el cuerpo del bucle o desenrollarlo en un factor de dos o más. El objetivo es aumentar la saturación del ancho de banda de la memoria y las instrucciones, lo que lleva a un mejor rendimiento. El desenrollado de urdimbre es otra técnica que se puede utilizar para optimizar bucles en CUDA, donde se realizan múltiples operaciones mediante un solo bloque de subproceso. El desenrollado completo es posible cuando se conoce el número de iteraciones del bucle en tiempo de compilación. Implica replicar el cuerpo del bucle para cada iteración, lo que permite operaciones de reducción más eficientes. En general, el desenrollado de bucles es una poderosa técnica de optimización que puede mejorar significativamente el rendimiento de los bucles en diversos escenarios informáticos.
En el contexto de la programación CUDA, la reducción con deformación desenrollada se refiere a una técnica utilizada para optimizar el rendimiento de los núcleos de reducción. El objetivo es reducir la sobrecarga de instrucción y crear instrucciones más independientes para programar, lo que lleva a una mayor saturación de la instrucción y el ancho de banda de la memoria.
La técnica consiste en desenrollar las últimas 6 iteraciones del bucle de reducción cuando quedan 32 hilos o menos (una sola urdimbre). Al desenrollar el bucle, se evita la ejecución de la lógica de control de bucle y sincronización de subprocesos. Esto se logra utilizando la función __syncthreads() para la sincronización dentro del bloque y declarando la variable vmem como volátil para garantizar el acceso correcto a la memoria.
Desenrollar la deformación de esta manera permite más operaciones concurrentes y ayuda a ocultar instrucciones o latencia de memoria, lo que en última instancia mejora el rendimiento general del núcleo de reducción.
El desenrollado completo es una técnica utilizada para mejorar el rendimiento en bucles donde el número de iteraciones se conoce en tiempo de compilación. Al desenrollar el bucle, se reduce el número de veces que se comprueba el estado del bucle, lo que da como resultado menos instrucciones y una mayor eficiencia. En CUDA, se puede aplicar un desenrollado completo a los bucles de reducción, donde el recuento de iteraciones del bucle se basa en la dimensión del bloque de hilo. Al desenrollar el bucle de reducción, se pueden agregar más operaciones simultáneas al proceso, lo que lleva a una mayor saturación de las instrucciones y del ancho de banda de la memoria. Esto se puede lograr desenrollando el bucle en diferentes niveles, como desenrollando 8 o desenrollando urdimbre. El rendimiento de diferentes estrategias de desarrollo se puede comparar utilizando métricas como el rendimiento de lectura de la memoria del dispositivo.

## Dynamic Parallelism
El paralelismo dinámico es una característica de CUDA que permite la creación de paralelismo dentro de una aplicación GPU en puntos arbitrarios de un kernel. Permite un enfoque más jerárquico de la concurrencia, donde el paralelismo se puede expresar en múltiples niveles dentro de un núcleo de GPU. Esta característica hace que los algoritmos recursivos sean más transparentes y más fáciles de entender, ya que la decisión de cuántos bloques y cuadrículas crear en una GPU se puede posponer hasta el tiempo de ejecución. El paralelismo dinámico también reduce la necesidad de transferir control de ejecución y datos entre el host y el dispositivo, ya que las decisiones de configuración de lanzamiento se pueden tomar en tiempo de ejecución mediante subprocesos que se ejecutan en el dispositivo.
El concepto de ejecución anidada en CUDA permite la invocación recursiva de funciones del kernel. En el libro se proporciona como ejemplo la función del núcleo "nestedHelloWorld". Cada hilo en el kernel imprime "Hello World" y luego verifica si debe terminar. Si el número de subprocesos en la capa anidada es mayor que uno, el subproceso 0 invoca recursivamente una cuadrícula secundaria con la mitad de subprocesos. Se muestra el resultado del programa del kernel anidado, lo que demuestra la ejecución recursiva y la profundidad de cada invocación.

# Global Memory
El modelo de memoria CUDA unifica sistemas de memoria de dispositivo y host separados y expone la jerarquía de memoria completa para que pueda controlar explícitamente la ubicación de los datos para un rendimiento óptimo. El acceso y la administración de la memoria son partes importantes de cualquier lenguaje de programación, y el modelo de memoria CUDA le permite lograr una latencia y un ancho de banda óptimos dado el subsistema de memoria del hardware.
Las aplicaciones suelen seguir el principio de localidad, lo que sugiere que acceden a una porción relativamente pequeña y localizada de su espacio de direcciones en cualquier momento. Hay dos tipos de localidad: localidad temporal (localidad en el tiempo) y localidad espacial (localidad en el espacio). El modelo de memoria CUDA aprovecha estas localidades mediante el uso de una jerarquía de memoria de memoria de latencia progresivamente menor pero de menor capacidad.
Diferentes patrones de acceso a la memoria global pueden afectar el rendimiento de su kernel. Al analizar estos patrones de acceso, puede aprender cómo utilizar la memoria global de manera eficiente desde su kernel. El modelo de memoria CUDA le permite controlar explícitamente la ubicación de los datos y optimizar los patrones de acceso a la memoria para un mejor rendimiento.
El modelo de memoria CUDA clasifica la memoria en dos categorías: programable y no programable. La memoria programable permite un control explícito sobre la ubicación de los datos, mientras que la memoria no programable se basa en técnicas automáticas para la optimización del rendimiento. En la jerarquía de memoria de la CPU, la caché L1 y la caché L2 son ejemplos de memoria no programable. Por otro lado, el modelo de memoria CUDA expone varios tipos de memoria programable, como registros, memoria compartida, memoria local, memoria constante, memoria de textura y memoria global. Cada tipo de memoria tiene su propio alcance, duración y comportamiento de almacenamiento en caché.

##  MEMORY MANAGEMENT
La gestión de la memoria en la programación CUDA es similar a la programación en C, con la responsabilidad adicional de gestionar explícitamente el movimiento de datos entre el host y el dispositivo. Si bien NVIDIA está trabajando para unificar el espacio de memoria del host y del dispositivo, todavía se requiere el movimiento manual de datos para la mayoría de las aplicaciones.
El modelo de programación CUDA supone un sistema heterogéneo con un host y un dispositivo, cada uno con su propio espacio de memoria independiente. Las funciones del kernel operan en el espacio de memoria del dispositivo y el tiempo de ejecución de CUDA proporciona funciones para asignar y desasignar memoria del dispositivo.
Puede asignar memoria global en el host usando la función **cudaMalloc**. Esta función asigna una cantidad específica de bytes de memoria global en el dispositivo y devuelve la ubicación de esa memoria. La memoria asignada está adecuadamente alineada para cualquier tipo de variable.
Para inicializar la memoria global asignada, puede utilizar la función **cudaMemset**. Esta función llena el número especificado de bytes en la memoria del dispositivo con un valor específico.
Una vez que una aplicación ya no utiliza una parte de la memoria global asignada, se puede desasignar utilizando la función **cudaFree**. Esta función libera la memoria global a la que apunta el puntero especificado.
Es importante tener en cuenta que la asignación y desasignación de memoria del dispositivo son operaciones costosas, por lo que se recomienda reutilizar la memoria del dispositivo siempre que sea posible para minimizar el impacto en el rendimiento general.
La memoria fijada es más costosa de asignar y desasignar que la memoria paginable, pero proporciona un mayor rendimiento de transferencia para grandes transferencias de datos. La aceleración lograda cuando se utiliza memoria fijada en relación con la memoria paginable depende de la capacidad informática del dispositivo. Por ejemplo, en los dispositivos Fermi generalmente resulta beneficioso utilizar memoria fija cuando se transfieren más de 10 MB de datos.
La agrupación de muchas transferencias pequeñas en una transferencia más grande mejora el rendimiento porque reduce la sobrecarga por transferencia. Las transferencias de datos entre el host y el dispositivo a veces pueden superponerse con la ejecución del kernel. Debe minimizar o superponer las transferencias de datos entre el host y el dispositivo siempre que sea posible.
La memoria de copia cero es un tipo de técnica de gestión de memoria utilizada en la programación CUDA. Se refiere a la memoria fijada (no paginable) que está asignada al espacio de direcciones del dispositivo. Esto permite compartir datos entre el host y el dispositivo sin la necesidad de transferencias de datos explícitas. 
La memoria de copia cero se puede crear utilizando la función **cudaHostAlloc** y es ventajosa en arquitecturas integradas donde las CPU y GPU comparten la memoria principal. Sin embargo, para sistemas discretos con dispositivos conectados a través del bus PCIe, la memoria de copia cero puede causar una degradación significativa del rendimiento y debe usarse con cuidado.
La memoria unificada es una característica introducida en CUDA 6.0 que simplifica la administración de la memoria en el modelo de programación CUDA. Crea un grupo de memoria administrada donde se puede acceder a cada asignación tanto en la CPU como en la GPU con la misma dirección de memoria. El sistema subyacente migra automáticamente datos en el espacio de memoria unificado entre el host y el dispositivo, lo que hace que el movimiento de datos sea transparente para la aplicación.
La memoria unificada ofrece un modelo de "puntero único a datos", similar a la memoria de copia cero, pero con un rendimiento mejorado. Desacopla la memoria y los espacios de ejecución, lo que permite que los datos se migren de forma transparente bajo demanda al host o dispositivo para mejorar la localidad y el rendimiento. Esto simplifica el código de la aplicación y proporciona migración automática de datos y eliminación de punteros duplicados.
Sin embargo, es importante tener en cuenta que la memoria unificada depende de la compatibilidad con el direccionamiento virtual unificado (UVA), que proporciona un único espacio de direcciones de memoria virtual para todos los procesadores del sistema. UVA y Unified Memory son tecnologías diferentes, y UVA no migra datos automáticamente entre ubicaciones físicas como lo hace Unified Memory.

## MEMORY ACCESS PATTERNS
Los patrones de acceso a la memoria son una consideración importante para optimizar el rendimiento del kernel en la programación CUDA. El modelo de ejecución CUDA emite y ejecuta instrucciones por warp, y las operaciones de memoria también se emiten por warp. El patrón de acceso a la memoria depende de la distribución de las direcciones de memoria dentro de un warp y se puede clasificar en diferentes patrones, como acceso alineado y combinado.
**Acceso alineado y fusionado**
Los accesos a memoria alineados ocurren cuando la primera dirección de una transacción de memoria es un múltiplo par de la granularidad de la caché que se utiliza (32 bytes para la caché L2 o 128 bytes para la caché L1). Los accesos a la memoria fusionados ocurren cuando los 32 subprocesos en una deformación acceden a una porción contigua de memoria. Los accesos a memoria alineados y fusionados son ideales para maximizar el rendimiento de la memoria global.
**Cargas almacenadas en caché y sin caché**
Las cargas de memoria se pueden almacenar en caché o sin caché. Las cargas almacenadas en caché pasan a través de la caché L1 y son atendidas por transacciones de memoria del dispositivo con la granularidad de una línea de caché L1 (128 bytes). Las cargas no almacenadas en caché no pasan a través de la caché L1 y se realizan con la granularidad de los segmentos de memoria (32 bytes). Las cargas no almacenadas en caché pueden ser más detalladas y pueden conducir a una mejor utilización del bus para accesos a memoria desalineados o no fusionados.
**Optimización del acceso a la memoria**
Para optimizar el acceso a la memoria, es importante organizar las operaciones de la memoria para que estén alineadas y fusionadas. Los accesos a la memoria alineados se pueden lograr asegurando que la primera dirección de una transacción de memoria esté alineada con la granularidad de la caché. Los accesos a la memoria fusionados se pueden lograr asegurando que todos los subprocesos en un warp accedan a una porción de memoria. Al optimizar los patrones de acceso a la memoria, puede maximizar la eficiencia de las transacciones de memoria y mejorar el rendimiento general del kernel.

## WHAT BANDWIDTH CAN A KERNEL ACHIEVE?
Al analizar el rendimiento del kernel, es importante considerar la latencia de la memoria y el ancho de banda de la memoria. La latencia de la memoria se refiere al tiempo que lleva satisfacer una solicitud de memoria, mientras que el ancho de banda de la memoria se refiere a la velocidad a la que se puede acceder a la memoria del dispositivo. El documento menciona dos métodos para mejorar el rendimiento del kernel: ocultar la latencia de la memoria maximizando el número de warps que se ejecutan simultáneamente y maximizar la eficiencia del ancho de banda de la memoria alineando y fusionando los accesos a la memoria.
El documento también analiza el concepto de ancho de banda teórico, que es el ancho de banda máximo que se puede lograr con el hardware disponible. Para un Fermi M2090 con ECC desactivado, el ancho de banda de memoria teórico máximo del dispositivo es 177,6 GB/s. El ancho de banda efectivo, por otro lado, es el ancho de banda medido que realmente alcanza un núcleo. El documento proporciona ejemplos de ancho de banda efectivo para diferentes núcleos, que van del 33,09% al 70,76% del ancho de banda máximo teórico.
Además, el documento explora varias técnicas de ajuste para ajustar el ancho de banda del kernel. Menciona la importancia de la disposición de la memoria y los patrones de acceso warp al afectar el ancho de banda. También analiza el impacto del tamaño de bloque en el paralelismo y proporciona ejemplos de ancho de banda efectivo logrado con diferentes tamaños de bloque.
En resumen, el ancho de banda alcanzable de un kernel depende de factores como la latencia de la memoria, la eficiencia del ancho de banda de la memoria, la disposición de la memoria, los patrones de acceso warp y el tamaño del bloque. Al optimizar estos factores, es posible lograr un buen rendimiento incluso con patrones de acceso inherentemente imperfectos.

# Conclusión
El tercer capítulo del libro se centra en el modelo de ejecución CUDA. Comienza discutiendo el desarrollo de kernels con un enfoque basado en perfiles, enfatizando la importancia de comprender la naturaleza de la ejecución warp. El capítulo también explora formas de exponer un mayor paralelismo a la GPU y proporciona pautas para dominar las heurísticas de configuración de bloques y cuadrículas. Además, cubre varias métricas y eventos de rendimiento de CUDA, así como paralelismo dinámico y ejecución anidada. En general, este capítulo ofrece información valiosa sobre cómo optimizar las configuraciones de lanzamiento del kernel y el rendimiento desde una perspectiva de hardware.
En el Capítulo 4, exploramos el modelo de memoria CUDA y aprendimos cómo administrar la memoria CUDA. Discutimos la programación con memoria global y examinamos los patrones de acceso a la memoria global. También probamos el diseño de datos de la memoria global y analizamos las implicaciones de rendimiento de diferentes patrones de acceso a la memoria.
Un concepto importante que cubrimos en este capítulo es la importancia de maximizar el rendimiento de la memoria global. Aprendimos que para mejorar la utilización del ancho de banda, debemos intentar maximizar la cantidad de accesos simultáneos a la memoria en vuelo y la utilización de bytes que viajan entre la memoria global y la memoria en el chip.
También analizamos los beneficios de la Memoria Unificada, una característica introducida en CUDA 6.0. La memoria unificada simplifica la programación CUDA al eliminar la necesidad de transferencias de datos explícitas entre el host y el dispositivo. Sin embargo, prioriza la coherencia sobre el desempeño.

Me agrado mas este libro pues lo siento más detallado que el anterior y menos saturado que el libro pasado que lo sentí con demasiados ejemplos y siendo muy pesado de leer

# Referencia:
S. Cook, CUDA Programming: A Developer's Guide to Parallel Computing with GPUs, John Wiley & Sons, Inc., 2013.
